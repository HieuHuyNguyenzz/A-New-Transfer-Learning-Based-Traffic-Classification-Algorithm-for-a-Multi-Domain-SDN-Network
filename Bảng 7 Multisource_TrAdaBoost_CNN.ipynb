{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPJIJ87EKtwvP4l96fxeWSe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7gAcSWLoyyyR"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime\n","import copy\n","from sklearn import tree\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import TimeDistributed, Conv1D, MaxPool1D, Flatten, LSTM, Dense, AveragePooling1D, Dropout, Conv2D, MaxPool2D\n","from keras.models import Sequential\n","from keras.regularizers import l2\n","import sklearn.metrics as metrics\n","from pyarrow import feather"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Nbs9XpL2CGu","executionInfo":{"status":"ok","timestamp":1717929476551,"user_tz":-420,"elapsed":21371,"user":{"displayName":"Duy Quang","userId":"08397240359579863284"}},"outputId":"2e913522-5136-4617-f3b4-bb475722ccf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def data_processing(df, NUM_FEATURES):\n","   y_train = df['Label']\n","   flow_id = df['flow_id']\n","\n","   df = df/255\n","\n","   X_train = df.drop(['Label', 'flow_id'], axis=1)\n","   X_train = X_train.to_numpy()\n","\n","   X_train = X_train.reshape(-1, 20, NUM_FEATURES)\n","   y_train = y_train.to_numpy()\n","\n","   y_train = y_train.reshape(-1,20)[:,-1]\n","   return X_train, y_train\n","\n","def DoiNhan(label: np.array):\n","  for i in range(len(label)):\n","    if label[i] == 14 or label[i] == 16 or label[i] == 13:\n","      label[i] = 2\n","    elif label[i] == 11 or label[i] == 5 or label[i] == 10:\n","      label[i] = 1\n","    elif label[i] == 4 or label[i] == 8 or label[i] == 9 or label[i] == 1 or label[i] == 7:\n","      label[i] = 0"],"metadata":{"id":"s8YC7oYs8iZo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def MultiSourceTrAdaBoost(trans_S, Multi_trans_A, label_S, Multi_label_A, test, N=3, NUM_CLASSES=3):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    trans_S : feature matrix of same-distribution training data\n","\n","    Multi_trans_A : dict, feature matrix of diff-distribution training data\n","    label_S : label of same-distribution training data, -1 or 1\n","\n","    Multi_label_A : dict, label of diff-distribution training data, -1 or 1\n","    e.g.,\n","    Multi_label_A = {\n","    'label_A_1' :  label_1 ,\n","    'label_A_2' : label_2 ,\n","    ......\n","    }\n","    label_1 : label of diff-distribution training dataset 1, -1 or 1\n","    label_1 : label of diff-distribution training dataset 2, -1 or 1\n","\n","    test : feature matrix of test data\n","\n","    N : int, default=3\n","    the number of weak estimators\n","\n","    \"\"\"\n","    # prepare trans_A\n","    trans_A = list(Multi_trans_A.values())[0]\n","\n","    if len(Multi_trans_A) == 1:\n","        pass\n","    else:\n","        for i in range(len(Multi_trans_A)-1):\n","            p = i + 1\n","            trans_A = np.concatenate((trans_A, list(Multi_trans_A.values())[p]), axis=0)\n","    # prepare label_A\n","    label_A = list(Multi_label_A.values())[0]\n","    if len(Multi_label_A) == 1:\n","        pass\n","    else:\n","        for i in range(len(Multi_label_A)-1):\n","            p = i + 1\n","            label_A = np.concatenate((label_A, list(Multi_label_A.values())[p]), axis=0)\n","\n","    trans_data = np.concatenate((trans_A, trans_S), axis=0)\n","    trans_label = np.concatenate((label_A, label_S), axis=0)\n","\n","    row_A = trans_A.shape[0]\n","    row_S = trans_S.shape[0]\n","    row_T = test.shape[0]\n","\n","    if N >= row_A:\n","        print('The maximum of iterations should be smaller than ', row_A)\n","\n","    test_data = np.concatenate((trans_data, test), axis=0)\n","\n","    # Initialize the weights\n","    weights_A = np.ones([row_A, 1]) / row_A\n","    weights_S = np.ones([row_S, 1]) / row_S\n","    # one-dim column in the shape of ((row_A+row_S),1), column vector\n","    weights = np.concatenate((weights_A, weights_S), axis=0)\n","\n","    alpha_S = 1/(1+np.sqrt(2*np.log(row_S + row_T)/N))\n","\n","\n","    # Save prediction labels and bata_t\n","    alpha_T = np.zeros([1, N])\n","    result_label = np.ones([row_A + row_S + row_T, N])\n","    # output label\n","    predict = np.zeros([row_T])\n","    print ('params initial finished.')\n","    print('='*60)\n","\n","    trans_data = np.asarray(trans_data, order='C')\n","    trans_label = np.asarray(trans_label, order='C')\n","    test_data = np.asarray(test_data, order='C')\n","\n","    for i in range(N):\n","        weights = calculate_ratio_weight(weights)\n","\n","        result_label[:, i], error_rate , Source_index, start = Multi_train_classifier(Multi_trans_A, label_S,trans_data, trans_label, test_data, weights,row_A,row_S)\n","        # Avoiding overfitting\n","        if error_rate >= 0.667:\n","            error_rate = 0.6;\n","\n","        if error_rate <= 1e-10:\n","            N = i\n","            break\n","\n","        alpha_T[0, i] = np.log((1 - error_rate) / error_rate) + np.log(NUM_CLASSES - 1)\n","        C_T = NUM_CLASSES * (1 - error_rate)\n","        print('Iter {}-th result :'.format(i))\n","        print('The {}-th diff-distribution training dataset is chosen to transfer'.format(Source_index))\n","        print('error rate :', error_rate, '|| alpha_T :', np.log((1 - error_rate) / error_rate) + np.log(NUM_CLASSES - 1) )\n","        print('-'*60)\n","\n","        # Changing the data weights of same-distribution training data\n","        for j in range(row_S):\n","          loss_t = np.sum(result_label[row_A + j, i] != label_S[j])\n","          weights[row_A + j] = weights[row_A + j] * np.exp(alpha_T[0, i] * loss_t)\n","        # Changing the data weights of diff-distribution training data\n","        for j in range( len(list(Multi_trans_A.values())[Source_index]) ):\n","            loc = start + j\n","            loss_ = np.sum(result_label[loc, i] != label_A[loc])\n","            weights[loc] = C_T * weights[loc] * np.exp(alpha_S * loss_)\n","\n","    for i in range(row_T):\n","      predict[i] = result_label[row_A + row_S + i, :][np.argmax(alpha_T[0, :])]\n","\n","    print(predict)\n","    print(predict.shape)\n","\n","    print(\"MultiSourceTrAdaBoost is done\")\n","    print('='*60)\n","    print('The prediction labels of test data are :')\n","    print(predict)\n","    return predict\n","\n","\n","def calculate_ratio_weight(weights):\n","    total = np.sum(weights)\n","    return np.asarray(weights / total, order='C')\n","\n","\n","# def train_classifier(trans_data, trans_label, test_data, ratio_weight):\n","#     clf = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth = 2, max_features=\"log2\", splitter=\"best\",random_state=0)\n","#     clf.fit(trans_data, trans_label, sample_weight=ratio_weight[:, 0])\n","#     return clf.predict(test_data)\n","\n","# def train_LSTM(trans_data, trans_label, test_data, ratio_weight):\n","\n","#     # Define model architecture\n","#     model = Sequential()\n","#     model.add(LSTM(64, input_shape=(trans_data.shape[1], 1),return_sequences=True))\n","#     # model.add(Dropout(0.2))\n","#     model.add(LSTM(32,activation='relu'))\n","#     model.add(Dense(16, activation=\"relu\"))\n","#     model.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","#     # model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","#     # history = model.fit(trans_data, trans_label, epochs=NUM_EPOCHS, sample_weight=ratio_weight[:, 0])\n","\n","#     model.compile(optimizer=tf.keras.optimizers.Adam(\n","#     learning_rate=client_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n","#     history = model.fit(trans_data, trans_label, epochs=NUM_EPOCHS, sample_weight=ratio_weight[:, 0], )\n","\n","#     # y_pred = model.predict(test_data)\n","\n","#     predictions = model.predict(test_data)\n","#     flow_pred = np.argmax(predictions, axis=-1)\n","#     return flow_pred\n","\n","def train_classifier(trans_data, trans_label, test_data, ratio_weight):\n","    NUM_EPOCHS = 2\n","    client_lr = 3e-4\n","    BATCH_SIZE = 32\n","\n","    model = Sequential()\n","\n","    model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='Same',\n","                     activation='relu', input_shape=(20,128,1)))\n","    model.add(Conv2D(filters=64 , kernel_size=(5, 5), padding='Same',\n","                     activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='Same',\n","                     activation='relu'))\n","    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n","                     activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='Same',\n","                     activation='relu'))\n","    model.add(Conv2D(filters=16, kernel_size=(3, 3), padding='Same',\n","                     activation='relu'))\n","    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","\n","    model.add(Flatten())\n","    model.add(Dense(256, activation=\"relu\"))\n","    model.add(Dropout(0.1))\n","    model.add(Dense(3, activation=\"softmax\"))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(\n","    learning_rate=client_lr), loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","\n","    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n","    model.fit(trans_data, trans_label, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n","                    shuffle=True, sample_weight=ratio_weight[:, 0])\n","\n","    predictions = model.predict(test_data, verbose=2, batch_size=BATCH_SIZE)\n","\n","    flow_pred = np.argmax(predictions, axis=-1)\n","\n","    return flow_pred\n","\n","def Multi_train_classifier(Multi_trans_A,label_S, trans_data, trans_label, test_data, weights,row_A,row_S):\n","    _result_label = np.ones([len(test_data), len(Multi_trans_A)])\n","    error_record = []\n","    start_record = []\n","    start = 0\n","    for item in range(len(Multi_trans_A)):\n","        start_record.append(start)\n","        sub_dataset = list(Multi_trans_A.values())[item]\n","        data_dim = len(sub_dataset)\n","        # train a classifier with the 'item'-th data source\n","        _trans_data = np.concatenate((trans_data[start : start + data_dim], trans_data[row_A:row_A + row_S]), axis=0)\n","        _trans_label = np.concatenate((trans_label[start : start + data_dim], trans_label[row_A:row_A + row_S]), axis=0)\n","        _ratio_weight = np.concatenate((weights[start : start + data_dim], weights[row_A:row_A + row_S]), axis=0)\n","        _result_label[:, item] = train_classifier(_trans_data, _trans_label, test_data, _ratio_weight)\n","        start += data_dim\n","        # cal error rate\n","        _error_rate = calculate_error_rate(label_S, _result_label[row_A:row_A + row_S, item],weights[row_A:row_A + row_S, :])\n","\n","        error_record.append(_error_rate)\n","    error_record = np.array(error_record)\n","    # choise the best classifier\n","    classifier_index = np.random.choice(np.flatnonzero(error_record == error_record.min()))\n","    return _result_label[:,classifier_index], error_record[classifier_index], classifier_index,start_record[classifier_index]\n","\n","def calculate_error_rate(label_R, label_P, weight):\n","    total = np.sum(weight)\n","    loss = np.sum(label_P != label_R)\n","    return np.sum(weight[:, 0] / total * loss)"],"metadata":{"id":"3OLVpxZP2Iyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_test():\n","  path = '/content/drive/MyDrive/Fixed_data/'\n","  test_name = 'd1test'\n","  train_name ='d1train'\n","\n","  diff = feather.read_feather('/content/drive/MyDrive/Fixed_data/domain2.feather')\n","  diff_distribution, diff_distribution_label = data_processing(diff, 128)\n","  DoiNhan(diff_distribution_label)\n","\n","  Multi_trans_A = {\n","    'trans_A_1' : diff_distribution\n","\n","    }\n","  Multi_label_A = {\n","    'label_A_1' :  diff_distribution_label\n","    }\n","\n","  print('source data initial finished')\n","  for i in range(1, 5):\n","    print('Start training with ' + str(i) + '% train data and ' + str(100 - i) + '% test data')\n","    same = feather.read_feather(path + train_name + str(i))\n","    df_tg = feather.read_feather(path + test_name + str(i))\n","\n","    same_distribution, same_distribution_label = data_processing(same, 128)\n","    tg, tg_label = data_processing(df_tg, 128)\n","\n","    DoiNhan(same_distribution_label)\n","    DoiNhan(tg_label)\n","\n","    trans_S = same_distribution\n","    label_S = same_distribution_label\n","    test = tg\n","    label_T = tg_label\n","\n","    predict = MultiSourceTrAdaBoost(trans_S, Multi_trans_A, label_S, Multi_label_A, test)\n","    print(metrics.classification_report(predict, label_T))"],"metadata":{"id":"Jjq1VRyN2CEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_and_test()"],"metadata":{"id":"t0Oj6pIANVRi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"039449df-78cf-4cc3-b5a6-31d54d0d8e32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["source data initial finished\n","Start training with 1% train data and 99% test data\n","params initial finished.\n","============================================================\n","Epoch 1/2\n","198/198 [==============================] - 612s 3s/step - loss: 1.6456e-04 - sparse_categorical_accuracy: 0.4998\n","Epoch 2/2\n","198/198 [==============================] - 605s 3s/step - loss: 1.2456e-04 - sparse_categorical_accuracy: 0.6958\n","474/474 - 381s - 381s/epoch - 805ms/step\n","Iter 0-th result :\n","The 0-th diff-distribution training dataset is chosen to transfer\n","error rate : 0.6 || alpha_T : 0.287682072451781\n","------------------------------------------------------------\n","Epoch 1/2\n"," 36/198 [====>.........................] - ETA: 8:03 - loss: 1.4399e-04 - sparse_categorical_accuracy: 0.3047"]}]},{"cell_type":"code","source":["while True:\n","  pass"],"metadata":{"id":"HpUa92p32CAF"},"execution_count":null,"outputs":[]}]}